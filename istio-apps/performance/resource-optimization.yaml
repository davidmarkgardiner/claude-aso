---
# Horizontal Pod Autoscaling for Service Mesh Components
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo-hpa
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-optimization
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo-v1
  minReplicas: 2
  maxReplicas: 50
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metrics - requests per second
  - type: Pods
    pods:
      metric:
        name: istio_requests_per_second
        selector:
          matchLabels:
            destination_service_name: podinfo
      target:
        type: AverageValue
        averageValue: "100"
  # Network I/O based scaling
  - type: External
    external:
      metric:
        name: istio_request_bytes_per_second
        selector:
          matchLabels:
            service: podinfo
      target:
        type: AverageValue
        averageValue: "1M"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 10  # Scale down by max 10% of current replicas
        periodSeconds: 60
      - type: Pods
        value: 2   # Scale down by max 2 pods
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60   # 1 minute
      policies:
      - type: Percent
        value: 50  # Scale up by max 50% of current replicas
        periodSeconds: 60
      - type: Pods
        value: 5   # Scale up by max 5 pods
        periodSeconds: 60

---
# Vertical Pod Autoscaling for Istio Proxy
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: istio-proxy-vpa
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-optimization
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo-v1
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    # Main application container
    - containerName: podinfo
      minAllowed:
        cpu: 10m
        memory: 32Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
    # Istio sidecar proxy
    - containerName: istio-proxy
      minAllowed:
        cpu: 10m
        memory: 64Mi
      maxAllowed:
        cpu: 500m
        memory: 512Mi
      controlledResources: ["cpu", "memory"]

---
# Performance Tuning EnvoyFilter
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: performance-optimization
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-tuning
spec:
  workloadSelector:
    labels:
      app: podinfo
  configPatches:
  # HTTP/2 optimization
  - applyTo: HTTP_CONNECTION_MANAGER
    match:
      context: SIDECAR_INBOUND
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
    patch:
      operation: MERGE
      value:
        http2_protocol_options:
          # Enable HTTP/2 for better performance
          max_concurrent_streams: 100
          initial_stream_window_size: 1048576  # 1MB
          initial_connection_window_size: 10485760  # 10MB
        # Connection pooling
        common_http_protocol_options:
          idle_timeout: 60s
          headers_timeout: 10s
        # Request buffering
        request_timeout: 30s
        stream_idle_timeout: 300s
        
  # Optimize connection pooling
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_OUTBOUND
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.adaptive_concurrency
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.http.adaptive_concurrency.v3.AdaptiveConcurrency
          concurrency_limit:
            # Adaptive concurrency control
            gradient_controller:
              sample_aggregate_percentile:
                value: 95
              concurrency_limit_params:
                max_gradient: 2.0
                max_concurrency_limit: 1000
                concurrency_update_interval: 0.1s
          enabled:
            default_value: true
            runtime_key: adaptive_concurrency_enabled

---
# Circuit Breaker Optimization
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: performance-circuit-breaker
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-optimization
spec:
  host: podinfo.tenant-a.svc.cluster.local
  trafficPolicy:
    # Connection pool settings for performance
    connectionPool:
      tcp:
        maxConnections: 100
        connectTimeout: 10s
        keepAliveTime: 600s
        keepAliveInterval: 60s
        keepAliveProbes: 3
        noDelay: true
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 1000
        maxRequestsPerConnection: 10
        maxRetries: 3
        idleTimeout: 60s
        h2UpgradePolicy: UPGRADE  # Prefer HTTP/2
    
    # Optimized circuit breaker settings
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 20
      # Performance-focused split external traffic
      splitExternalLocalOriginErrors: true
      # Consecutive gateway failures
      consecutive5xxErrors: 5
      consecutiveGatewayErrors: 5
    
    # Load balancing optimization
    loadBalancer:
      simple: LEAST_REQUEST  # Better for varied request sizes
      localityLbSetting:
        enabled: true
        # Prefer local zone for performance
        distribute:
        - from: "region/*/zone/*"
          to:
            "region/westeurope/zone/1": 80
            "region/westeurope/zone/2": 20
        # Fast failover
        failover:
        - from: "region/westeurope"
          to: "region/eastus"

---
# Resource Quotas for Performance Isolation
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-performance-quota
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-isolation
spec:
  hard:
    # Compute resources
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    
    # Network resources
    services.loadbalancers: "2"
    services.nodeports: "0"
    
    # Storage
    persistentvolumeclaims: "10"
    requests.storage: 100Gi
    
    # Objects
    pods: "50"
    replicationcontrollers: "10"
    secrets: "50"
    configmaps: "50"

---
# Limit Ranges for Performance Consistency
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-a-performance-limits
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-consistency
spec:
  limits:
  # Container limits
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu: "10m"
      memory: "32Mi"
  
  # Pod limits
  - type: Pod
    max:
      cpu: "4"
      memory: "4Gi"
  
  # Persistent Volume Claims
  - type: PersistentVolumeClaim
    min:
      storage: "1Gi"
    max:
      storage: "100Gi"

---
# Network Policies for Performance Isolation
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-a-performance-isolation
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: performance-network-isolation
spec:
  podSelector: {}  # Apply to all pods in namespace
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow from Istio system
  - from:
    - namespaceSelector:
        matchLabels:
          name: aks-istio-system
  # Allow from same namespace
  - from:
    - namespaceSelector:
        matchLabels:
          name: tenant-a
  # Allow monitoring
  - from:
    - namespaceSelector:
        matchLabels:
          name: shared-services
    ports:
    - protocol: TCP
      port: 15090  # Istio proxy metrics
  egress:
  # Allow to same namespace
  - to:
    - namespaceSelector:
        matchLabels:
          name: tenant-a
  # Allow to shared services (monitoring, etc.)
  - to:
    - namespaceSelector:
        matchLabels:
          name: shared-services
  # Allow to external services
  - to:
    - namespaceSelector:
        matchLabels:
          name: external-services
  # Allow DNS
  - to: []
    ports:
    - protocol: UDP
      port: 53

---
# Performance Monitoring Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: performance-dashboard
  namespace: shared-services
  labels:
    deployment-agent: istio-engineer
    grafana_dashboard: "1"
data:
  performance-optimization.json: |
    {
      "dashboard": {
        "title": "Istio Performance Optimization",
        "tags": ["istio", "performance"],
        "panels": [
          {
            "title": "Request Rate (RPS)",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(istio_requests_total[5m])) by (destination_service_name)",
                "legendFormat": "{{ destination_service_name }} RPS"
              }
            ]
          },
          {
            "title": "Response Time P95",
            "type": "graph", 
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (destination_service_name, le))",
                "legendFormat": "{{ destination_service_name }} P95"
              }
            ]
          },
          {
            "title": "CPU Utilization",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(container_cpu_usage_seconds_total{container=\"istio-proxy\"}[5m])) by (pod) * 100",
                "legendFormat": "{{ pod }} CPU %"
              }
            ]
          },
          {
            "title": "Memory Utilization",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(container_memory_working_set_bytes{container=\"istio-proxy\"}) by (pod) / 1024 / 1024",
                "legendFormat": "{{ pod }} Memory (MB)"
              }
            ]
          },
          {
            "title": "Circuit Breaker Status",
            "type": "stat",
            "targets": [
              {
                "expr": "envoy_cluster_outlier_detection_ejections_active",
                "legendFormat": "Active Ejections"
              }
            ]
          },
          {
            "title": "Connection Pool Status",
            "type": "graph",
            "targets": [
              {
                "expr": "envoy_cluster_upstream_cx_active",
                "legendFormat": "Active Connections"
              },
              {
                "expr": "envoy_cluster_upstream_cx_overflow",
                "legendFormat": "Connection Overflow"
              }
            ]
          }
        ]
      }
    }

---
# PodDisruptionBudget for High Availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: podinfo-pdb
  namespace: tenant-a
  labels:
    deployment-agent: istio-engineer
    component: high-availability
spec:
  minAvailable: 2  # Always keep at least 2 pods running
  selector:
    matchLabels:
      app: podinfo